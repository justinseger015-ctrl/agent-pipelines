# Loop Agents v3 Implementation Plan

> **Status: âœ… COMPLETE** - All phases implemented, tests passing (295 tests), code review fixes applied.

## Overview

This plan details the implementation of the Loop Agents v3 architecture, transforming the current system from a variable-based template resolution to a unified context manifest approach. The goal is to make the system so standardized that agents can create new pipelines without making mistakes.

**Key Changes:**
1. Context manifest (`context.json`) replacing 9+ template variables
2. Universal status format (`decision: continue|stop|error`)
3. Engine-side output snapshots (remove `output.mode` config)
4. Keep progress file (no changes needed)
5. Explicit input selection (`inputs.from`, `inputs.select`)
6. Fail fast (remove retry logic, write clear failure state)

**Approach:** Test-Driven Development with existing infrastructure

---

## ğŸš¨ MANDATORY: Test-Driven Development

> **THIS IS NOT OPTIONAL.** Every phase includes a "âš ï¸ TDD REQUIREMENT" section.
> You MUST complete those steps BEFORE writing implementation code.

### The TDD Contract

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FOR EVERY PHASE:                                               â”‚
â”‚                                                                 â”‚
â”‚  1. WRITE TESTS FIRST     â†’ Create test file with assertions   â”‚
â”‚  2. VERIFY TESTS FAIL     â†’ Run tests, confirm they fail       â”‚
â”‚  3. IMPLEMENT CODE        â†’ Write the minimum to pass tests    â”‚
â”‚  4. VERIFY TESTS PASS     â†’ Run tests, confirm they pass       â”‚
â”‚  5. RUN FULL TEST SUITE   â†’ Ensure no regressions              â”‚
â”‚                                                                 â”‚
â”‚  âŒ WRONG: "Let me implement status.sh first..."               â”‚
â”‚  âœ… RIGHT: "Let me write test_status.sh first..."              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Why This Matters

- **Tests define the contract** - They specify what the code should do before it exists
- **Tests catch regressions** - Changes that break existing behavior are caught immediately
- **Tests enable refactoring** - Confident changes because tests verify correctness
- **Tests document intent** - Future readers understand expected behavior

### Quick Reference: Test Files by Phase

| Phase | Test File | Tests |
|-------|-----------|-------|
| 0 | `test_infrastructure.sh`, `test_mock.sh`, `test_fixtures.sh`, `test_validation.sh` | âœ… Complete |
| 1 | `test_context.sh` | âœ… Complete |
| 2 | `test_status.sh`, `test_completions.sh` | âœ… Complete |
| 3 | `test_engine_snapshots.sh` | âœ… Complete (unit tests; integration tests in Phase 6) |
| 4 | `test_inputs.sh` | Write FIRST |
| 5 | `test_failure.sh` | Write FIRST |
| 6 | `test_regression.sh`, `test_engine_integration.sh` | Write FIRST |

---

## Existing Test Infrastructure

The codebase already has validation tools we can leverage:

| Tool | Command | Purpose |
|------|---------|---------|
| **Lint** | `./scripts/run.sh lint` | Validate all loop/pipeline configs |
| **Lint specific** | `./scripts/run.sh lint loop work` | Validate one loop |
| **Dry-run** | `./scripts/run.sh dry-run loop work auth` | Preview execution without Claude |
| **Status** | `./scripts/run.sh status <session>` | Check session state |
| **Test** | `./scripts/run.sh test` | Run all tests |
| **Test specific** | `./scripts/run.sh test status` | Run specific test file |

**Validation rules defined in `scripts/lib/validate.sh`:**
- L001-L013: Loop validation rules
- P001-P012: Pipeline validation rules

### TDD Workflow (Detailed)

For each phase:

```bash
# Step 1: Write test file
# Create scripts/tests/test_{module}.sh with failing tests

# Step 2: Verify tests fail
./scripts/run.sh test {module}
# Expected: Tests fail or error (code doesn't exist yet)

# Step 3: Implement code
# Write scripts/lib/{module}.sh

# Step 4: Verify tests pass
./scripts/run.sh test {module}
# Expected: All tests pass

# Step 5: Full regression check
./scripts/run.sh test
./scripts/run.sh lint
# Expected: All tests pass, all configs valid
```

---

## Current State Analysis

### Files to Modify

| File | Lines | Changes Needed |
|------|-------|----------------|
| `scripts/engine.sh` | 600 | Major rewrite of iteration loop, add context.json generation |
| `scripts/lib/resolve.sh` | 145 | Replace with context.json generator, deprecate old variables |
| `scripts/lib/state.sh` | 197 | Add status.json integration, update state schema |
| `scripts/lib/completions/plateau.sh` | 47 | Read `decision` from status.json instead of parsing output |
| `scripts/lib/completions/beads-empty.sh` | 29 | Minor: integrate with new status format |
| `scripts/lib/completions/fixed-n.sh` | 19 | Minor: integrate with new status format |
| `scripts/lib/progress.sh` | 58 | No changes (kept as-is) |
| `scripts/lib/parse.sh` | 43 | Deprecate in favor of status.json reading |
| `scripts/loops/*/loop.yaml` | 5 files | Update schema (new termination block) |
| `scripts/loops/*/prompt.md` | 4 files | Update to use `${CTX}` and write status.json |
| `scripts/pipelines/*.yaml` | 3 files | Add `inputs` configuration |

### Current Variable Usage (to be replaced)

From `resolve.sh`, the current variables are:
- `${SESSION}` / `${SESSION_NAME}` â†’ `context.session`
- `${ITERATION}` â†’ `context.iteration`
- `${INDEX}` â†’ Not needed (0-indexed iteration)
- `${PROGRESS}` / `${PROGRESS_FILE}` â†’ `context.paths.progress`
- `${OUTPUT}` â†’ `context.paths.output`
- `${OUTPUT_PATH}` â†’ Merge with `context.paths.output`
- `${INPUTS}` / `${INPUTS.stage-name}` â†’ `context.inputs.from_stage.*`

---

## Implementation Phases

### Phase 0: Test Harness Enhancement âœ… COMPLETE

**Goal:** Extend existing validation to support v3 and add mock execution for integration tests.

**Completed 2025-01-11:**
- Created `scripts/lib/test.sh` with 15+ assertion functions
- Created `scripts/lib/mock.sh` for mock Claude execution
- Added `./scripts/run.sh test` command
- Created fixtures for all 5 loops (work, improve-plan, elegance, idea-wizard, refine-beads)
- Created 4 test files with 90 passing tests
- Verified: `./scripts/run.sh test` and `./scripts/run.sh lint` both pass

#### 0.1 Add v3 Validation Rules

**File:** `scripts/lib/validate.sh` (extend)

Add new validation rules for v3 schema:

```bash
# V3 validation rules (add to KNOWN_VARS)
KNOWN_VARS_V3="CTX PROGRESS OUTPUT STATUS"

# L014: v3 stages should use termination block
# L015: v3 stages should not use output_parse (deprecated)
# L016: prompts should reference ${STATUS} for status output
# L017: prompts should reference ${CTX} for context

validate_loop_v3() {
  # ... check for new termination block
  # ... check for deprecated output_parse
  # ... check prompt uses new variables
}
```

#### 0.2 Add Mock Execution Mode

**File:** `scripts/lib/mock.sh` (new file)

```bash
#!/bin/bash
# Mock execution for testing

MOCK_MODE=false
MOCK_DIR=""

# Enable mock mode with fixture directory
enable_mock() {
  MOCK_MODE=true
  MOCK_DIR=$1
}

# Get mock response for iteration
get_mock_response() {
  local iteration=$1
  local fixture_file="$MOCK_DIR/iteration-${iteration}.txt"

  if [ -f "$fixture_file" ]; then
    cat "$fixture_file"
  elif [ -f "$MOCK_DIR/default.txt" ]; then
    cat "$MOCK_DIR/default.txt"
  else
    # Generate minimal valid response
    echo '{"decision": "continue", "reason": "Mock response"}'
  fi
}
```

#### 0.3 Add Test Command

**File:** `scripts/run.sh` (extend)

```bash
test)
  source "$LIB_DIR/validate.sh"
  source "$LIB_DIR/mock.sh"
  shift
  # ... run with mock responses
  ;;
```

#### 0.4 Create Fixture Templates

**Directory:** `scripts/loops/*/fixtures/`

Create default fixtures for each loop type:

```
scripts/loops/work/fixtures/
â”œâ”€â”€ default.txt          # Default mock response
â””â”€â”€ status.json          # Expected status format

scripts/loops/improve-plan/fixtures/
â”œâ”€â”€ iteration-1.txt      # First iteration response
â”œâ”€â”€ iteration-2.txt      # Second iteration (plateau)
â””â”€â”€ status.json          # Expected status format
```

#### Success Criteria - Phase 0

- [ ] `./scripts/run.sh lint` passes with all current stages
- [ ] `./scripts/run.sh lint --v3` validates v3 schema rules
- [ ] `./scripts/run.sh test loop work --iterations 2` runs with mock responses
- [ ] Each loop has a `fixtures/` directory with default responses

---

### Phase 1: Context Manifest (`context.json`) âœ… COMPLETE

**Goal:** Replace 9+ template variables with a single structured JSON file.

**Completed 2025-01-11:**
- Created `scripts/lib/context.sh` with `generate_context()`, `build_inputs_json()`, `calculate_remaining_time()`
- Updated `scripts/lib/resolve.sh` to support v3 variables (`${CTX}`, `${STATUS}`) while maintaining backward compatibility
- Updated `scripts/engine.sh` to generate context.json before each iteration
- Added 25 tests in `scripts/tests/test_context.sh` validating context generation and resolution
- Verified: `./scripts/run.sh test context` passes, all 115 tests pass

#### 1.1 Create Context Generator

**File:** `scripts/lib/context.sh` (new file)

```bash
#!/bin/bash
# Context Manifest Generator
# Creates context.json for each iteration

# Generate context.json for an iteration
# Usage: generate_context "$session" "$iteration" "$stage_config" "$run_dir"
generate_context() {
  local session=$1
  local iteration=$2
  local stage_config=$3  # JSON object
  local run_dir=$4

  local stage_id=$(echo "$stage_config" | jq -r '.id // .name')
  local stage_idx=$(echo "$stage_config" | jq -r '.index // 0')
  local stage_template=$(echo "$stage_config" | jq -r '.template // .loop // ""')

  # Paths
  local stage_dir="$run_dir/stage-$(printf '%02d' $stage_idx)-$stage_id"
  local iter_dir="$stage_dir/iterations/$(printf '%03d' $iteration)"
  local progress_file="$stage_dir/progress.md"
  local output_file="$stage_dir/output.md"
  local status_file="$iter_dir/status.json"

  mkdir -p "$iter_dir"

  # Build inputs (from previous stage and previous iterations)
  local inputs_json=$(build_inputs_json "$run_dir" "$stage_config" "$iteration")

  # Build limits
  local max_iterations=$(echo "$stage_config" | jq -r '.max_iterations // 50')
  local started_at=$(jq -r '.started_at // ""' "$run_dir/state.json" 2>/dev/null)
  local remaining_seconds=$(calculate_remaining_time "$started_at" "$stage_config")

  # Generate context.json
  jq -n \
    --arg session "$session" \
    --arg pipeline "$(jq -r '.pipeline // ""' "$run_dir/state.json" 2>/dev/null)" \
    --arg stage_id "$stage_id" \
    --argjson stage_idx "$stage_idx" \
    --arg template "$stage_template" \
    --argjson iteration "$iteration" \
    --arg session_dir "$run_dir" \
    --arg stage_dir "$stage_dir" \
    --arg progress "$progress_file" \
    --arg output "$output_file" \
    --arg status "$status_file" \
    --argjson inputs "$inputs_json" \
    --argjson max_iterations "$max_iterations" \
    --argjson remaining "$remaining_seconds" \
    '{
      session: $session,
      pipeline: $pipeline,
      stage: {id: $stage_id, index: $stage_idx, template: $template},
      iteration: $iteration,
      paths: {
        session_dir: $session_dir,
        stage_dir: $stage_dir,
        progress: $progress,
        output: $output,
        status: $status
      },
      inputs: $inputs,
      limits: {
        max_iterations: $max_iterations,
        remaining_seconds: $remaining
      }
    }' > "$iter_dir/context.json"

  echo "$iter_dir/context.json"
}
```

**Functions to add:**
- `generate_context()` - Main context generator
- `build_inputs_json()` - Build inputs object based on `inputs.from` and `inputs.select`
- `calculate_remaining_time()` - Calculate remaining runtime

#### 1.2 Update Prompt Resolution

**File:** `scripts/lib/resolve.sh` (modify)

Replace the existing implementation:

```bash
#!/bin/bash
# Context-Based Variable Resolution (v3)
# Resolves only 4 convenience variables; full context via ${CTX}

resolve_prompt() {
  local template=$1
  local context_file=$2  # Path to context.json

  local resolved="$template"

  # Read context
  local ctx=$(cat "$context_file")

  # Resolve 4 convenience paths
  local progress=$(echo "$ctx" | jq -r '.paths.progress')
  local output=$(echo "$ctx" | jq -r '.paths.output')
  local status=$(echo "$ctx" | jq -r '.paths.status')

  resolved="${resolved//\$\{CTX\}/$context_file}"
  resolved="${resolved//\$\{PROGRESS\}/$progress}"
  resolved="${resolved//\$\{OUTPUT\}/$output}"
  resolved="${resolved//\$\{STATUS\}/$status}"

  # DEPRECATED: Keep old variables working during migration
  local session=$(echo "$ctx" | jq -r '.session')
  local iteration=$(echo "$ctx" | jq -r '.iteration')
  resolved="${resolved//\$\{SESSION\}/$session}"
  resolved="${resolved//\$\{SESSION_NAME\}/$session}"
  resolved="${resolved//\$\{ITERATION\}/$iteration}"
  resolved="${resolved//\$\{PROGRESS_FILE\}/$progress}"

  echo "$resolved"
}
```

#### 1.3 Update Engine to Generate Context

**File:** `scripts/engine.sh` (modify `run_stage` function)

Add context generation before prompt resolution:

```bash
# In run_stage(), before executing Claude:

# Generate context.json for this iteration
local context_file=$(generate_context "$session" "$i" "$stage_config_json" "$run_dir")

# Resolve prompt using context file
local resolved_prompt=$(resolve_prompt "$LOOP_PROMPT" "$context_file")
```

#### Success Criteria - Phase 1

- [x] `context.json` is generated in `iterations/NNN/` before each iteration
- [x] Prompts can use `${CTX}` to read the context file
- [x] Old variables (`${SESSION}`, `${ITERATION}`, etc.) still work (deprecated)
- [x] Test: Run `./scripts/run.sh test context` and verify all tests pass

---

### Phase 2: Universal Status Format âœ… COMPLETE

**Goal:** Every agent writes the same `status.json` format with `decision: continue|stop|error`.

**Completed 2025-01-11:**
- Created `scripts/lib/status.sh` with validation and management functions
- Fixed `assert_json_field_exists` bug in test.sh
- Created `scripts/tests/test_status.sh` (39 tests) and `scripts/tests/test_completions.sh` (9 tests)
- Updated all completion strategies to accept status_file parameter
- Updated engine.sh to pass status_file to check_completion and store decision in history
- Updated all prompts: improve-plan, elegance, work, idea-wizard, refine-beads
- All 47 tests pass, lint validation passes

---

#### âš ï¸ TDD REQUIREMENT: Write Tests First

**STOP. Before writing ANY implementation code, complete these steps:**

##### Step 0.1: Create `scripts/tests/test_status.sh`

```bash
#!/bin/bash
# Tests for status.sh - WRITE THIS FIRST

source "$SCRIPT_DIR/lib/test.sh"

#-------------------------------------------------------------------------------
# Status Validation Tests
#-------------------------------------------------------------------------------

test_validate_status_missing_file() {
  validate_status "/nonexistent/path.json" 2>/dev/null
  local result=$?
  assert_eq "1" "$result" "validate_status fails for missing file"
}

test_validate_status_valid_continue() {
  local tmp=$(mktemp)
  echo '{"decision": "continue", "reason": "test"}' > "$tmp"
  validate_status "$tmp"
  local result=$?
  rm -f "$tmp"
  assert_eq "0" "$result" "validate_status accepts 'continue'"
}

test_validate_status_valid_stop() {
  local tmp=$(mktemp)
  echo '{"decision": "stop", "reason": "test"}' > "$tmp"
  validate_status "$tmp"
  local result=$?
  rm -f "$tmp"
  assert_eq "0" "$result" "validate_status accepts 'stop'"
}

test_validate_status_valid_error() {
  local tmp=$(mktemp)
  echo '{"decision": "error", "reason": "test"}' > "$tmp"
  validate_status "$tmp"
  local result=$?
  rm -f "$tmp"
  assert_eq "0" "$result" "validate_status accepts 'error'"
}

test_validate_status_invalid_decision() {
  local tmp=$(mktemp)
  echo '{"decision": "invalid", "reason": "test"}' > "$tmp"
  validate_status "$tmp" 2>/dev/null
  local result=$?
  rm -f "$tmp"
  assert_eq "1" "$result" "validate_status rejects invalid decision"
}

test_validate_status_missing_decision() {
  local tmp=$(mktemp)
  echo '{"reason": "no decision field"}' > "$tmp"
  validate_status "$tmp" 2>/dev/null
  local result=$?
  rm -f "$tmp"
  assert_eq "1" "$result" "validate_status rejects missing decision"
}

test_get_status_decision() {
  local tmp=$(mktemp)
  echo '{"decision": "stop", "reason": "test"}' > "$tmp"
  local decision=$(get_status_decision "$tmp")
  rm -f "$tmp"
  assert_eq "stop" "$decision" "get_status_decision extracts decision"
}

test_get_status_reason() {
  local tmp=$(mktemp)
  echo '{"decision": "stop", "reason": "my reason here"}' > "$tmp"
  local reason=$(get_status_reason "$tmp")
  rm -f "$tmp"
  assert_eq "my reason here" "$reason" "get_status_reason extracts reason"
}

test_create_error_status() {
  local tmp=$(mktemp)
  create_error_status "$tmp" "Test error message"
  local decision=$(jq -r '.decision' "$tmp")
  local reason=$(jq -r '.reason' "$tmp")
  rm -f "$tmp"
  assert_eq "error" "$decision" "create_error_status sets decision=error"
  assert_eq "Test error message" "$reason" "create_error_status sets reason"
}

#-------------------------------------------------------------------------------
# Run Tests
#-------------------------------------------------------------------------------

run_test "validate_status missing file" test_validate_status_missing_file
run_test "validate_status valid continue" test_validate_status_valid_continue
run_test "validate_status valid stop" test_validate_status_valid_stop
run_test "validate_status valid error" test_validate_status_valid_error
run_test "validate_status invalid decision" test_validate_status_invalid_decision
run_test "validate_status missing decision" test_validate_status_missing_decision
run_test "get_status_decision" test_get_status_decision
run_test "get_status_reason" test_get_status_reason
run_test "create_error_status" test_create_error_status
```

##### Step 0.2: Create `scripts/tests/test_completions.sh`

```bash
#!/bin/bash
# Tests for completion strategies - WRITE THIS FIRST

source "$SCRIPT_DIR/lib/test.sh"
source "$SCRIPT_DIR/lib/mock.sh"

#-------------------------------------------------------------------------------
# Plateau Completion Tests
#-------------------------------------------------------------------------------

test_plateau_needs_min_iterations() {
  # Setup: iteration 1, min_iterations=2
  # Expected: return 1 (don't stop yet)
}

test_plateau_single_stop_not_enough() {
  # Setup: iteration 2, current says stop, previous said continue
  # Expected: return 1 (need consensus)
}

test_plateau_two_consecutive_stops() {
  # Setup: iteration 3, current says stop, previous said stop
  # Expected: return 0 (consensus reached)
}

test_plateau_reads_from_status_json() {
  # Setup: status.json with decision: stop
  # Expected: reads decision correctly, not from output parsing
}

#-------------------------------------------------------------------------------
# Beads-Empty Completion Tests
#-------------------------------------------------------------------------------

test_beads_empty_with_remaining() {
  # Setup: bd ready returns items
  # Expected: return 1 (keep going)
}

test_beads_empty_with_none() {
  # Setup: bd ready returns nothing
  # Expected: return 0 (complete)
}

test_beads_empty_ignores_error_status() {
  # Setup: status.json has decision: error
  # Expected: return 1 (don't complete on error)
}
```

##### Step 0.3: Verify Tests Fail

```bash
# Run the new tests - they MUST fail because status.sh doesn't exist yet
./scripts/run.sh test status
# Expected: "source: scripts/lib/status.sh: No such file or directory" or similar

./scripts/run.sh test completions
# Expected: Tests fail because completion strategies don't read status.json yet
```

**âœ… CHECKPOINT: Tests written and failing. NOW proceed to implementation.**

---

#### 2.1 Define Status Schema

**File:** `scripts/lib/status.sh` (new file)

```bash
#!/bin/bash
# Status File Management
# Handles the universal status.json format

# Validate status.json
# Usage: validate_status "$status_file"
# Returns: 0 if valid, 1 if invalid
validate_status() {
  local status_file=$1

  if [ ! -f "$status_file" ]; then
    echo "Error: Status file not found: $status_file" >&2
    return 1
  fi

  local decision=$(jq -r '.decision // "missing"' "$status_file" 2>/dev/null)

  case "$decision" in
    continue|stop|error) return 0 ;;
    missing)
      echo "Error: Status file missing 'decision' field" >&2
      return 1
      ;;
    *)
      echo "Error: Invalid decision value: $decision (must be continue|stop|error)" >&2
      return 1
      ;;
  esac
}

# Read status decision
# Usage: get_status_decision "$status_file"
get_status_decision() {
  local status_file=$1
  jq -r '.decision // "continue"' "$status_file" 2>/dev/null
}

# Read status reason
# Usage: get_status_reason "$status_file"
get_status_reason() {
  local status_file=$1
  jq -r '.reason // ""' "$status_file" 2>/dev/null
}

# Create error status (when agent crashes or times out)
# Usage: create_error_status "$status_file" "$error_message"
create_error_status() {
  local status_file=$1
  local error=$2
  local timestamp=$(date -u +%Y-%m-%dT%H:%M:%SZ)

  jq -n \
    --arg error "$error" \
    --arg ts "$timestamp" \
    '{
      decision: "error",
      reason: $error,
      summary: "Iteration failed due to error",
      work: {items_completed: [], files_touched: []},
      errors: [$error],
      timestamp: $ts
    }' > "$status_file"
}
```

#### 2.2 Update Completion Strategies

**File:** `scripts/lib/completions/plateau.sh` (rewrite)

```bash
#!/bin/bash
# Completion strategy: judgment (plateau)
# Requires N consecutive agents to write decision: stop

check_completion() {
  local session=$1
  local state_file=$2
  local status_file=$3  # Now receives status file path

  # Get configurable consensus count (default 2)
  local consensus_needed=${CONSENSUS:-2}
  local min_iterations=${MIN_ITERATIONS:-2}

  # Read current iteration
  local iteration=$(get_state "$state_file" "iteration")

  # Must hit minimum iterations first
  if [ "$iteration" -lt "$min_iterations" ]; then
    return 1
  fi

  # Read current decision from status.json
  local decision=$(get_status_decision "$status_file")

  if [ "$decision" = "stop" ]; then
    # Count consecutive "stop" decisions from history
    local history=$(get_history "$state_file")
    local consecutive=1

    # Check previous iterations for consecutive stops
    for ((i = iteration - 1; i >= 1 && consecutive < consensus_needed; i--)); do
      local prev_decision=$(echo "$history" | jq -r ".[$((i-1))].decision // \"continue\"")
      if [ "$prev_decision" = "stop" ]; then
        ((consecutive++))
      else
        break
      fi
    done

    if [ "$consecutive" -ge "$consensus_needed" ]; then
      local reason=$(get_status_reason "$status_file")
      echo "Consensus reached: $consecutive consecutive agents agree to stop"
      echo "  Reason: $reason"
      return 0
    else
      echo "Stop suggested but not confirmed ($consecutive/$consensus_needed needed)"
      return 1
    fi
  fi

  return 1
}
```

**File:** `scripts/lib/completions/beads-empty.sh` (update)

```bash
#!/bin/bash
# Completion strategy: queue (beads-empty)
# Complete when external queue is empty

check_completion() {
  local session=$1
  local state_file=$2
  local status_file=$3

  # Check if agent reported error
  local decision=$(get_status_decision "$status_file" 2>/dev/null)
  if [ "$decision" = "error" ]; then
    return 1  # Don't complete on error
  fi

  local remaining=$(bd ready --label="loop/$session" 2>/dev/null | grep -c "^" || echo "0")

  if [ "$remaining" -eq 0 ]; then
    echo "All beads complete"
    return 0
  fi

  return 1
}
```

#### 2.3 Update Stage Schema

**Current format (v2):**
```yaml
name: improve-plan
completion: plateau
min_iterations: 2
output_parse: "plateau:PLATEAU reasoning:REASONING"
```

**New format (v3):**
```yaml
name: improve-plan
description: Iteratively improve a plan document

termination:
  type: judgment          # judgment | queue | fixed
  min_iterations: 2
  consensus: 2            # Consecutive "stop" decisions needed

guardrails:
  max_iterations: 50
  max_runtime_seconds: 7200
```

#### 2.4 Update All Prompt Templates

Each prompt must instruct the agent to write `status.json`:

```markdown
## Status Output

After completing your work, write your status to `${STATUS}`:

\`\`\`json
{
  "decision": "continue",  // or "stop" or "error"
  "reason": "Brief explanation of why",
  "summary": "One paragraph describing what happened this iteration",
  "work": {
    "items_completed": [],
    "files_touched": ["path/to/file.ts"]
  },
  "errors": []
}
\`\`\`

**Decision guide:**
- `"continue"` - More work needed
- `"stop"` - Work is complete, no more improvements possible
- `"error"` - Something went wrong that needs investigation
```

#### Success Criteria - Phase 2

- [ ] All prompts instruct agents to write `status.json`
- [ ] Completion strategies read from `status.json` instead of parsing output
- [ ] Stage configs use new `termination` block
- [ ] Test: Run `./scripts/run.sh improve-plan test 5` and verify `status.json` files are created
- [ ] Test: Verify plateau detection works with new format

---

### Phase 3: Engine-Side Output Snapshots âœ… COMPLETE

**Goal:** Engine automatically saves iteration outputs to `iterations/NNN/output.md`.

**Completed 2026-01-11:**
- Created `scripts/tests/test_engine_snapshots.sh` with 18 tests
- Updated `engine.sh` to save output snapshot to `iterations/NNN/output.md` after each iteration
- Updated `engine.sh` to create error status if agent doesn't write `status.json`
- All 65 tests pass (47 existing + 18 new), lint validation passes

**TDD Note:** The unit tests verify behavior patterns using helper functions rather than exercising the actual engine code path. True engine integration tests (that run `run_stage` with mock Claude) are deferred to Phase 6 Step 0.4 (`test_engine_integration.sh`).

---

#### âš ï¸ TDD REQUIREMENT: Write Tests First

**STOP. Before writing ANY implementation code, complete these steps:**

##### Step 0.1: Create `scripts/tests/test_engine_snapshots.sh`

```bash
#!/bin/bash
# Tests for engine output snapshots - WRITE THIS FIRST

source "$SCRIPT_DIR/lib/test.sh"
source "$SCRIPT_DIR/lib/mock.sh"

#-------------------------------------------------------------------------------
# Output Snapshot Tests
#-------------------------------------------------------------------------------

test_iteration_creates_output_snapshot() {
  # Setup: Run mock iteration
  # Expected: iterations/001/output.md exists
  local tmp=$(create_test_dir)
  # ... setup mock execution ...
  assert_file_exists "$tmp/stage-00-test/iterations/001/output.md" "Output snapshot created"
  cleanup_test_dir "$tmp"
}

test_multiple_iterations_preserve_history() {
  # Setup: Run 3 mock iterations
  # Expected: iterations/001/, 002/, 003/ all have output.md
  local tmp=$(create_test_dir)
  # ... setup mock execution ...
  assert_file_exists "$tmp/stage-00-test/iterations/001/output.md" "Iteration 1 preserved"
  assert_file_exists "$tmp/stage-00-test/iterations/002/output.md" "Iteration 2 preserved"
  assert_file_exists "$tmp/stage-00-test/iterations/003/output.md" "Iteration 3 preserved"
  cleanup_test_dir "$tmp"
}

test_missing_status_creates_error() {
  # Setup: Mock iteration that doesn't write status.json
  # Expected: Engine creates error status automatically
  local tmp=$(create_test_dir)
  # ... setup mock execution without status.json ...
  local decision=$(jq -r '.decision' "$tmp/stage-00-test/iterations/001/status.json")
  assert_eq "error" "$decision" "Missing status triggers error status"
  cleanup_test_dir "$tmp"
}

#-------------------------------------------------------------------------------
# Run Tests
#-------------------------------------------------------------------------------

run_test "Iteration creates output snapshot" test_iteration_creates_output_snapshot
run_test "Multiple iterations preserve history" test_multiple_iterations_preserve_history
run_test "Missing status creates error" test_missing_status_creates_error
```

##### Step 0.2: Verify Tests Fail

```bash
# Run the new tests - they MUST fail because engine doesn't create snapshots yet
./scripts/run.sh test engine_snapshots
# Expected: Assertions fail - output.md files not created
```

**âœ… CHECKPOINT: Tests written and failing. NOW proceed to implementation.**

---

#### 3.1 Update Engine Iteration Loop

**File:** `scripts/engine.sh` (modify `run_stage` function)

After Claude execution, copy output to iteration directory:

```bash
# In run_stage(), after execute_claude:

# Save output snapshot to iteration directory
local iter_dir="$stage_dir/iterations/$(printf '%03d' $i)"
if [ -f "$output_file" ]; then
  cp "$output_file" "$iter_dir/output.md"
fi

# Save status.json to iteration (agent writes to ${STATUS})
# Engine validates it exists and is well-formed
local status_file="$iter_dir/status.json"
if [ ! -f "$status_file" ]; then
  # Agent didn't write status - create error status
  create_error_status "$status_file" "Agent did not write status.json"
fi
```

#### 3.2 Remove `output.mode` from Schema

**Files to update:**
- `scripts/loops/*/loop.yaml` - Remove any `output:` blocks with `mode:`
- `scripts/engine.sh` - Remove mode-based output handling

**New simplified schema:**
```yaml
# Stage only specifies WHERE, not HOW
output: docs/plan-${SESSION}.md   # Tracked in repo
# or
output: .claude                    # Internal only (default)
```

#### Success Criteria - Phase 3

- [x] Every iteration has `iterations/NNN/output.md` snapshot
- [x] `output.mode` config removed from all stages (was never implemented)
- [x] Stage config only specifies output location, not versioning
- [x] Test: Run multiple iterations and verify output history is preserved

---

### Phase 4: Explicit Input Selection âœ… COMPLETE

**Goal:** Stages explicitly declare what inputs they want from previous stages.

**Completed 2026-01-11:**
- Created `scripts/tests/test_inputs.sh` with 21 tests (TDD approach)
- Verified `build_inputs_json` in `context.sh` already implements full functionality
- Updated `engine.sh` to read `inputs` config from pipeline YAML and pass to context generator
- Updated all 3 pipeline files (quick-refine, full-refine, deep-refine) with `inputs` block
- Updated `validate.sh` to recognize v3 variables (`${CTX}`, `${STATUS}`)
- All 47 tests pass, lint validation passes

---

#### âš ï¸ TDD REQUIREMENT: Write Tests First

**STOP. Before writing ANY implementation code, complete these steps:**

##### Step 0.1: Create `scripts/tests/test_inputs.sh`

```bash
#!/bin/bash
# Tests for input selection - WRITE THIS FIRST

source "$SCRIPT_DIR/lib/test.sh"
source "$SCRIPT_DIR/lib/context.sh"

#-------------------------------------------------------------------------------
# Input Resolution Tests
#-------------------------------------------------------------------------------

test_inputs_from_previous_stage_latest() {
  # Setup: Stage 2 with inputs.from=stage1, inputs.select=latest
  # Stage 1 has iterations 001, 002, 003
  # Expected: inputs.from_stage.stage1 contains only iteration 003 output
  local tmp=$(create_test_dir)
  mkdir -p "$tmp/stage-00-stage1/iterations/001"
  mkdir -p "$tmp/stage-00-stage1/iterations/002"
  mkdir -p "$tmp/stage-00-stage1/iterations/003"
  echo "output1" > "$tmp/stage-00-stage1/iterations/001/output.md"
  echo "output2" > "$tmp/stage-00-stage1/iterations/002/output.md"
  echo "output3" > "$tmp/stage-00-stage1/iterations/003/output.md"

  local config='{"id":"stage2","index":1,"inputs":{"from":"stage1","select":"latest"}}'
  local inputs=$(build_inputs_json "$tmp" "$config" 1)
  local count=$(echo "$inputs" | jq '.from_stage.stage1 | length')

  assert_eq "1" "$count" "select=latest returns single file"
  cleanup_test_dir "$tmp"
}

test_inputs_from_previous_stage_all() {
  # Setup: Stage 2 with inputs.from=stage1, inputs.select=all
  # Stage 1 has iterations 001, 002, 003
  # Expected: inputs.from_stage.stage1 contains all 3 outputs
  local tmp=$(create_test_dir)
  mkdir -p "$tmp/stage-00-stage1/iterations/001"
  mkdir -p "$tmp/stage-00-stage1/iterations/002"
  mkdir -p "$tmp/stage-00-stage1/iterations/003"
  echo "output1" > "$tmp/stage-00-stage1/iterations/001/output.md"
  echo "output2" > "$tmp/stage-00-stage1/iterations/002/output.md"
  echo "output3" > "$tmp/stage-00-stage1/iterations/003/output.md"

  local config='{"id":"stage2","index":1,"inputs":{"from":"stage1","select":"all"}}'
  local inputs=$(build_inputs_json "$tmp" "$config" 1)
  local count=$(echo "$inputs" | jq '.from_stage.stage1 | length')

  assert_eq "3" "$count" "select=all returns all files"
  cleanup_test_dir "$tmp"
}

test_inputs_default_is_latest() {
  # Setup: Stage 2 with inputs.from=stage1, NO select specified
  # Expected: Defaults to select=latest behavior
  local tmp=$(create_test_dir)
  mkdir -p "$tmp/stage-00-stage1/iterations/001"
  mkdir -p "$tmp/stage-00-stage1/iterations/002"
  echo "output1" > "$tmp/stage-00-stage1/iterations/001/output.md"
  echo "output2" > "$tmp/stage-00-stage1/iterations/002/output.md"

  local config='{"id":"stage2","index":1,"inputs":{"from":"stage1"}}'
  local inputs=$(build_inputs_json "$tmp" "$config" 1)
  local count=$(echo "$inputs" | jq '.from_stage.stage1 | length')

  assert_eq "1" "$count" "default select is latest (single file)"
  cleanup_test_dir "$tmp"
}

test_inputs_from_previous_iterations() {
  # Setup: Stage at iteration 3
  # Expected: from_previous_iterations contains iterations 1 and 2
  local tmp=$(create_test_dir)
  mkdir -p "$tmp/stage-00-current/iterations/001"
  mkdir -p "$tmp/stage-00-current/iterations/002"
  echo "iter1" > "$tmp/stage-00-current/iterations/001/output.md"
  echo "iter2" > "$tmp/stage-00-current/iterations/002/output.md"

  local config='{"id":"current","index":0}'
  local inputs=$(build_inputs_json "$tmp" "$config" 3)
  local count=$(echo "$inputs" | jq '.from_previous_iterations | length')

  assert_eq "2" "$count" "iteration 3 sees 2 previous iterations"
  cleanup_test_dir "$tmp"
}

test_inputs_nonexistent_stage() {
  # Setup: inputs.from references a stage that doesn't exist
  # Expected: from_stage is empty object, no error
  local tmp=$(create_test_dir)
  local config='{"id":"stage2","index":1,"inputs":{"from":"nonexistent"}}'
  local inputs=$(build_inputs_json "$tmp" "$config" 1)
  local from_stage=$(echo "$inputs" | jq '.from_stage')

  assert_eq "{}" "$from_stage" "nonexistent stage returns empty object"
  cleanup_test_dir "$tmp"
}

#-------------------------------------------------------------------------------
# Run Tests
#-------------------------------------------------------------------------------

run_test "Inputs from previous stage (latest)" test_inputs_from_previous_stage_latest
run_test "Inputs from previous stage (all)" test_inputs_from_previous_stage_all
run_test "Inputs default is latest" test_inputs_default_is_latest
run_test "Inputs from previous iterations" test_inputs_from_previous_iterations
run_test "Inputs nonexistent stage" test_inputs_nonexistent_stage
```

##### Step 0.2: Verify Tests Fail

```bash
# Run the new tests - they MUST fail because build_inputs_json doesn't exist yet
./scripts/run.sh test inputs
# Expected: "build_inputs_json: command not found" or assertions fail
```

**âœ… CHECKPOINT: Tests written and failing. NOW proceed to implementation.**

---

#### 4.1 Add Input Resolution to Context Generator

**File:** `scripts/lib/context.sh` (add function)

```bash
# Build inputs JSON based on pipeline config
# Usage: build_inputs_json "$run_dir" "$stage_config" "$iteration"
build_inputs_json() {
  local run_dir=$1
  local stage_config=$2
  local iteration=$3

  local inputs_from=$(echo "$stage_config" | jq -r '.inputs.from // ""')
  local inputs_select=$(echo "$stage_config" | jq -r '.inputs.select // "latest"')

  local from_stage="{}"
  local from_iterations="[]"

  # Collect from previous stage
  if [ -n "$inputs_from" ]; then
    local source_dir=$(find "$run_dir" -maxdepth 1 -type d -name "stage-*-$inputs_from" | head -1)

    if [ -d "$source_dir" ]; then
      case "$inputs_select" in
        all)
          # Get all iteration outputs
          from_stage=$(jq -n --arg name "$inputs_from" --arg dir "$source_dir" \
            '{($name): [($dir + "/iterations/*/output.md") | @sh]}')
          ;;
        latest)
          # Get only the latest output
          local latest=$(ls -1 "$source_dir/iterations" 2>/dev/null | sort -n | tail -1)
          if [ -n "$latest" ]; then
            from_stage=$(jq -n --arg name "$inputs_from" \
              --arg file "$source_dir/iterations/$latest/output.md" \
              '{($name): [$file]}')
          fi
          ;;
      esac
    fi
  fi

  # Collect from previous iterations of current stage
  local stage_idx=$(echo "$stage_config" | jq -r '.index // 0')
  local stage_id=$(echo "$stage_config" | jq -r '.id // .name')
  local current_stage_dir="$run_dir/stage-$(printf '%02d' $stage_idx)-$stage_id"

  if [ "$iteration" -gt 1 ] && [ -d "$current_stage_dir/iterations" ]; then
    from_iterations=$(find "$current_stage_dir/iterations" -name "output.md" -type f | \
      sort | head -$((iteration - 1)) | jq -R . | jq -s .)
  fi

  jq -n \
    --argjson from_stage "$from_stage" \
    --argjson from_iterations "$from_iterations" \
    '{from_stage: $from_stage, from_previous_iterations: $from_iterations}'
}
```

#### 4.2 Update Pipeline Schema

**Current format (v2):**
```yaml
stages:
  - name: ideas
    loop: idea-generator
    runs: 5
  - name: synthesize
    loop: synthesizer
    runs: 1
```

**New format (v3):**
```yaml
stages:
  - id: ideas
    template: idea-generator
    max_iterations: 5

  - id: synthesize
    template: synthesizer
    max_iterations: 1
    inputs:
      from: ideas
      select: all         # Get all 5 idea files

  - id: refine
    template: refiner
    max_iterations: 10
    inputs:
      from: synthesize
      select: latest      # Only need the most recent output
```

#### Success Criteria - Phase 4

- [x] Pipelines support `inputs.from` and `inputs.select`
- [x] Default is `select: latest`
- [x] `context.json` correctly populates `inputs.from_stage`
- [x] Test: 21 tests in `test_inputs.sh` verify all input selection functionality

---

### Phase 5: Fail Fast âœ… COMPLETE

**Goal:** Remove retry logic, fail immediately with clear error state.

**Completed 2026-01-11:**
- Created `scripts/tests/test_failure.sh` with 26 tests (TDD approach)
- Updated `mark_failed` in `state.sh` to create structured error object with `type`, `message`, `timestamp` and `resume_from`
- Updated `engine.sh` to fail immediately on Claude exit code != 0 (both loop and pipeline modes)
- Error state includes clear resume instructions for crash recovery
- All 193 tests pass, lint validation passes

---

#### âš ï¸ TDD REQUIREMENT: Write Tests First

**STOP. Before writing ANY implementation code, complete these steps:**

##### Step 0.1: Create `scripts/tests/test_failure.sh`

```bash
#!/bin/bash
# Tests for failure handling and resume - WRITE THIS FIRST

source "$SCRIPT_DIR/lib/test.sh"
source "$SCRIPT_DIR/lib/state.sh"
source "$SCRIPT_DIR/lib/lock.sh"

#-------------------------------------------------------------------------------
# Failure State Tests
#-------------------------------------------------------------------------------

test_mark_failed_sets_status() {
  local tmp=$(create_test_dir)
  local state_file="$tmp/state.json"
  echo '{"session":"test","iteration":3,"iteration_completed":2}' > "$state_file"

  mark_failed "$state_file" "Test error"
  local status=$(jq -r '.status' "$state_file")

  assert_eq "failed" "$status" "mark_failed sets status=failed"
  cleanup_test_dir "$tmp"
}

test_mark_failed_includes_error_details() {
  local tmp=$(create_test_dir)
  local state_file="$tmp/state.json"
  echo '{"session":"test","iteration":3,"iteration_completed":2}' > "$state_file"

  mark_failed "$state_file" "API timeout" "timeout"
  local error_type=$(jq -r '.error.type' "$state_file")
  local error_msg=$(jq -r '.error.message' "$state_file")

  assert_eq "timeout" "$error_type" "mark_failed includes error type"
  assert_eq "API timeout" "$error_msg" "mark_failed includes error message"
  cleanup_test_dir "$tmp"
}

test_mark_failed_sets_resume_from() {
  local tmp=$(create_test_dir)
  local state_file="$tmp/state.json"
  echo '{"session":"test","iteration":5,"iteration_completed":4}' > "$state_file"

  mark_failed "$state_file" "Crash"
  local resume_from=$(jq -r '.resume_from' "$state_file")

  assert_eq "5" "$resume_from" "resume_from = iteration_completed + 1"
  cleanup_test_dir "$tmp"
}

#-------------------------------------------------------------------------------
# Resume Tests
#-------------------------------------------------------------------------------

test_get_resume_iteration() {
  local tmp=$(create_test_dir)
  local state_file="$tmp/state.json"
  echo '{"session":"test","iteration":5,"iteration_completed":4}' > "$state_file"

  local resume=$(get_resume_iteration "$state_file")
  assert_eq "5" "$resume" "get_resume_iteration returns completed + 1"
  cleanup_test_dir "$tmp"
}

test_can_resume_failed_session() {
  local tmp=$(create_test_dir)
  local state_file="$tmp/state.json"
  echo '{"status":"failed","iteration":5,"iteration_completed":4}' > "$state_file"

  can_resume "$state_file"
  local result=$?
  assert_eq "0" "$result" "can_resume returns 0 for failed session"
  cleanup_test_dir "$tmp"
}

test_cannot_resume_completed_session() {
  local tmp=$(create_test_dir)
  local state_file="$tmp/state.json"
  echo '{"status":"complete","iteration":5,"iteration_completed":5}' > "$state_file"

  can_resume "$state_file"
  local result=$?
  assert_eq "1" "$result" "can_resume returns 1 for completed session"
  cleanup_test_dir "$tmp"
}

test_reset_for_resume_clears_error() {
  local tmp=$(create_test_dir)
  local state_file="$tmp/state.json"
  echo '{"status":"failed","error":{"message":"old error"}}' > "$state_file"

  reset_for_resume "$state_file"
  local status=$(jq -r '.status' "$state_file")
  local has_error=$(jq 'has("error")' "$state_file")

  assert_eq "running" "$status" "reset_for_resume sets status=running"
  # Note: May keep error for history, but status should be running
  cleanup_test_dir "$tmp"
}

#-------------------------------------------------------------------------------
# No Retry Logic Tests
#-------------------------------------------------------------------------------

test_no_retry_on_failure() {
  # This is more of an integration test
  # Verify engine doesn't retry - fails immediately
  # Would need mock execution to properly test
  skip_test "Integration test - verify manually"
}

#-------------------------------------------------------------------------------
# Run Tests
#-------------------------------------------------------------------------------

run_test "mark_failed sets status" test_mark_failed_sets_status
run_test "mark_failed includes error details" test_mark_failed_includes_error_details
run_test "mark_failed sets resume_from" test_mark_failed_sets_resume_from
run_test "get_resume_iteration" test_get_resume_iteration
run_test "can_resume failed session" test_can_resume_failed_session
run_test "cannot resume completed session" test_cannot_resume_completed_session
run_test "reset_for_resume clears error" test_reset_for_resume_clears_error
run_test "No retry on failure" test_no_retry_on_failure
```

##### Step 0.2: Verify Tests Fail (or identify gaps)

```bash
# Run the new tests
./scripts/run.sh test failure
# Some may pass (existing state.sh functions), some should fail (new requirements)
# Specifically: resume_from field, error.type structure may not exist yet
```

**âœ… CHECKPOINT: Tests written. NOW proceed to implementation to make failing tests pass.**

---

#### 5.1 Remove Retry Logic from Engine

**File:** `scripts/engine.sh` (modify)

Remove any retry counting or failure recovery loops. Replace with immediate failure:

```bash
# When Claude exits with error:
if [ $exit_code -ne 0 ]; then
  local error_msg="Claude process exited with code $exit_code"

  # Write error status
  create_error_status "$status_file" "$error_msg"

  # Update state with failure
  mark_failed "$state_file" "$error_msg"

  echo ""
  echo "Session failed at iteration $i"
  echo "Error: $error_msg"
  echo ""
  echo "To resume: ./scripts/run.sh loop $stage_type $session $max --resume"

  return 1
fi
```

#### 5.2 Update Stage Schema - Guardrails Only

**Remove from schema:**
- `max_failures`
- Any retry-related config

**Keep in schema:**
```yaml
guardrails:
  max_iterations: 50           # Hard stop
  max_runtime_seconds: 7200    # 2 hour timeout
```

#### 5.3 Improve Failure State

**File:** `scripts/lib/state.sh` (update `mark_failed`)

```bash
# Mark session as failed with detailed error
mark_failed() {
  local state_file=$1
  local error=$2
  local error_type=${3:-"unknown"}

  local timestamp=$(date -u +%Y-%m-%dT%H:%M:%SZ)
  local iteration=$(jq -r '.iteration // 0' "$state_file")
  local iteration_completed=$(jq -r '.iteration_completed // 0' "$state_file")

  jq --arg error "$error" \
     --arg type "$error_type" \
     --arg ts "$timestamp" \
     --argjson resume "$((iteration_completed + 1))" \
     '.status = "failed" |
      .failed_at = $ts |
      .error = {
        type: $type,
        message: $error,
        timestamp: $ts
      } |
      .resume_from = $resume' \
     "$state_file" > "$state_file.tmp" && mv "$state_file.tmp" "$state_file"
}
```

#### Success Criteria - Phase 5

- [x] No retry logic in engine
- [x] Failures write clear error state with `resume_from`
- [x] `--resume` correctly reads `resume_from` and continues
- [x] Test: 26 tests in `test_failure.sh` verify all failure handling functionality

---

### Phase 6: Migration & Cleanup âœ… COMPLETE

**Goal:** Remove deprecated code, update all stages, refresh documentation.

**Completed 2026-01-11:**
- Created `scripts/tests/test_regression.sh` with 27 tests for v3 schema validation
- Updated all 5 loop.yaml files to use v3 `termination` block:
  - work: `termination.type: queue`
  - improve-plan, elegance, refine-beads: `termination.type: judgment`, `consensus: 2`
  - idea-wizard: `termination.type: fixed`
- Updated all 4 prompt.md files to use v3 variables (`${CTX}`, `${PROGRESS}`, `${STATUS}`)
- Removed legacy `completion`, `output_parse` fields from all loop configs
- Removed legacy PLATEAU/REASONING output from prompts (kept status.json)
- Marked `scripts/lib/parse.sh` as DEPRECATED
- Updated `scripts/lib/validate.sh` to support v3 `termination` schema
- Updated `CLAUDE.md` with v3 architecture documentation:
  - New architecture diagram showing context.sh, status.sh
  - Updated template variables section with v3 preferred variables
  - Updated stage creation instructions with v3 schema
  - Updated termination strategies table
- All 220 tests pass, all 8 lint targets pass

---

#### âš ï¸ TDD REQUIREMENT: Regression Tests First

**STOP. Before making ANY migration changes, ensure test coverage exists:**

##### Step 0.1: Verify All Previous Phase Tests Pass

```bash
# ALL tests from phases 0-5 must pass before migration
./scripts/run.sh test
# Expected: All tests pass (status, completions, inputs, failure, etc.)
```

##### Step 0.2: Create `scripts/tests/test_regression.sh`

```bash
#!/bin/bash
# Regression tests for v3 migration - WRITE THIS FIRST

source "$SCRIPT_DIR/lib/test.sh"

#-------------------------------------------------------------------------------
# Stage Definition Tests (v3 schema)
#-------------------------------------------------------------------------------

test_work_stage_v3_schema() {
  local config=$(yaml_to_json "$LOOPS_DIR/work/loop.yaml")
  local term_type=$(echo "$config" | jq -r '.termination.type // empty')
  assert_eq "queue" "$term_type" "work stage uses termination.type=queue"
}

test_improve_plan_stage_v3_schema() {
  local config=$(yaml_to_json "$LOOPS_DIR/improve-plan/loop.yaml")
  local term_type=$(echo "$config" | jq -r '.termination.type // empty')
  local consensus=$(echo "$config" | jq -r '.termination.consensus // empty')
  assert_eq "judgment" "$term_type" "improve-plan uses termination.type=judgment"
  assert_eq "2" "$consensus" "improve-plan requires consensus=2"
}

test_elegance_stage_v3_schema() {
  local config=$(yaml_to_json "$LOOPS_DIR/elegance/loop.yaml")
  local term_type=$(echo "$config" | jq -r '.termination.type // empty')
  assert_eq "judgment" "$term_type" "elegance uses termination.type=judgment"
}

test_idea_wizard_stage_v3_schema() {
  local config=$(yaml_to_json "$LOOPS_DIR/idea-wizard/loop.yaml")
  local term_type=$(echo "$config" | jq -r '.termination.type // empty')
  assert_eq "fixed" "$term_type" "idea-wizard uses termination.type=fixed"
}

test_refine_beads_stage_v3_schema() {
  local config=$(yaml_to_json "$LOOPS_DIR/refine-beads/loop.yaml")
  local term_type=$(echo "$config" | jq -r '.termination.type // empty')
  assert_eq "judgment" "$term_type" "refine-beads uses termination.type=judgment"
}

#-------------------------------------------------------------------------------
# Prompt Variable Tests (v3 variables)
#-------------------------------------------------------------------------------

test_prompts_use_ctx_variable() {
  for loop_dir in "$LOOPS_DIR"/*/; do
    local prompt_file="$loop_dir/prompt.md"
    [ -f "$prompt_file" ] || continue
    local loop_name=$(basename "$loop_dir")
    local content=$(cat "$prompt_file")
    assert_contains "$content" '${CTX}' "$loop_name prompt uses \${CTX}"
  done
}

test_prompts_use_status_variable() {
  for loop_dir in "$LOOPS_DIR"/*/; do
    local prompt_file="$loop_dir/prompt.md"
    [ -f "$prompt_file" ] || continue
    local loop_name=$(basename "$loop_dir")
    local content=$(cat "$prompt_file")
    assert_contains "$content" '${STATUS}' "$loop_name prompt uses \${STATUS}"
  done
}

test_no_deprecated_output_parse() {
  for loop_dir in "$LOOPS_DIR"/*/; do
    local config_file="$loop_dir/loop.yaml"
    [ -f "$config_file" ] || continue
    local loop_name=$(basename "$loop_dir")
    local content=$(cat "$config_file")
    assert_not_contains "$content" "output_parse" "$loop_name has no deprecated output_parse"
  done
}

#-------------------------------------------------------------------------------
# Deprecated Code Removal Tests
#-------------------------------------------------------------------------------

test_parse_sh_marked_deprecated() {
  local content=$(cat "$SCRIPT_DIR/lib/parse.sh" 2>/dev/null || echo "")
  if [ -n "$content" ]; then
    assert_contains "$content" "DEPRECATED" "parse.sh is marked as deprecated"
  fi
}

#-------------------------------------------------------------------------------
# Run Tests
#-------------------------------------------------------------------------------

run_test "work stage v3 schema" test_work_stage_v3_schema
run_test "improve-plan stage v3 schema" test_improve_plan_stage_v3_schema
run_test "elegance stage v3 schema" test_elegance_stage_v3_schema
run_test "idea-wizard stage v3 schema" test_idea_wizard_stage_v3_schema
run_test "refine-beads stage v3 schema" test_refine_beads_stage_v3_schema
run_test "Prompts use CTX variable" test_prompts_use_ctx_variable
run_test "Prompts use STATUS variable" test_prompts_use_status_variable
run_test "No deprecated output_parse" test_no_deprecated_output_parse
run_test "parse.sh marked deprecated" test_parse_sh_marked_deprecated
```

##### Step 0.3: Verify Regression Tests Fail (before migration)

```bash
# Run regression tests - they SHOULD fail because migration not done yet
./scripts/run.sh test regression
# Expected: Schema tests fail (old completion: instead of termination:)
# Expected: Prompt variable tests fail (old ${SESSION} instead of ${CTX})
```

**âœ… CHECKPOINT: Regression tests written and failing. NOW proceed to migration.**

---

##### Step 0.4: Create Engine Integration Tests

**Background:** Phase 3 tests verified behavior patterns but tested local helper functions rather than the actual engine code path. This step adds true integration tests that run the engine with mock Claude.

**File:** `scripts/tests/test_engine_integration.sh`

```bash
#!/bin/bash
# Engine integration tests - verifies engine actually creates expected files
# These tests run the engine with mock responses to verify file creation

source "$SCRIPT_DIR/lib/test.sh"
source "$SCRIPT_DIR/lib/mock.sh"

#-------------------------------------------------------------------------------
# Engine Output Snapshot Integration Tests
#-------------------------------------------------------------------------------

test_engine_creates_output_snapshot() {
  local tmp=$(create_test_dir)
  local run_dir="$tmp/pipeline-runs/test-session"
  mkdir -p "$run_dir"

  # Enable mock mode with a fixture that writes status.json
  enable_mock "$SCRIPT_DIR/loops/work/fixtures"

  # Run one iteration through the engine
  # Note: This requires mock infrastructure to intercept Claude calls
  MOCK_MODE=true run_stage "work" "test-session" 1 "$run_dir" 0 1 2>/dev/null || true

  # Verify the engine created output snapshot
  local iter_dir="$run_dir/stage-00-work/iterations/001"
  assert_file_exists "$iter_dir/output.md" "Engine saved output snapshot to iteration directory"

  cleanup_test_dir "$tmp"
}

test_engine_creates_error_status_when_missing() {
  local tmp=$(create_test_dir)
  local run_dir="$tmp/pipeline-runs/test-session"
  mkdir -p "$run_dir"

  # Enable mock mode with fixture that does NOT write status.json
  enable_mock "$tmp/no-status-fixture"
  mkdir -p "$tmp/no-status-fixture"
  echo "Mock output without status" > "$tmp/no-status-fixture/default.txt"

  # Run one iteration
  MOCK_MODE=true run_stage "work" "test-session" 1 "$run_dir" 0 1 2>/dev/null || true

  # Verify engine created error status
  local status_file="$run_dir/stage-00-work/iterations/001/status.json"
  assert_file_exists "$status_file" "Engine created status.json"

  local decision=$(jq -r '.decision' "$status_file" 2>/dev/null)
  assert_eq "error" "$decision" "Engine set decision=error when agent didn't write status"

  cleanup_test_dir "$tmp"
}

test_engine_preserves_agent_status() {
  local tmp=$(create_test_dir)
  local run_dir="$tmp/pipeline-runs/test-session"
  mkdir -p "$run_dir"

  # Enable mock mode with fixture that DOES write status.json with decision=stop
  enable_mock "$SCRIPT_DIR/loops/improve-plan/fixtures"

  # Run one iteration
  MOCK_MODE=true run_stage "improve-plan" "test-session" 1 "$run_dir" 0 1 2>/dev/null || true

  # Verify engine preserved agent's status (not overwritten with error)
  local status_file="$run_dir/stage-00-improve-plan/iterations/001/status.json"
  local decision=$(jq -r '.decision' "$status_file" 2>/dev/null)

  # Agent fixture should have written continue or stop, not error
  assert_not_eq "error" "$decision" "Engine preserved agent's status (not overwritten)"

  cleanup_test_dir "$tmp"
}

#-------------------------------------------------------------------------------
# Run Tests
#-------------------------------------------------------------------------------

run_test "Engine creates output snapshot" test_engine_creates_output_snapshot
run_test "Engine creates error status when missing" test_engine_creates_error_status_when_missing
run_test "Engine preserves agent status" test_engine_preserves_agent_status
```

**Note:** These tests require the mock infrastructure to properly intercept Claude calls during `run_stage`. If the current mock system doesn't support this, the tests should be marked as `skip_test` with a comment explaining the limitation, and manual verification should be documented.

**Verification:**
```bash
./scripts/run.sh test engine_integration
# Expected: Tests pass OR are skipped with clear explanation
```

---

#### 6.1 Update All Stage Definitions

| Stage | Current | New |
|-------|---------|-----|
| `work` | `completion: beads-empty` | `termination: {type: queue}` |
| `improve-plan` | `completion: plateau` | `termination: {type: judgment, consensus: 2}` |
| `refine-beads` | `completion: plateau` | `termination: {type: judgment, consensus: 2}` |
| `idea-wizard` | `completion: fixed-n` | `termination: {type: fixed}` |
| `elegance` | `completion: plateau` | `termination: {type: judgment, consensus: 2}` |

#### 6.2 Update All Prompts

For each prompt in `scripts/loops/*/prompt.md`:

1. Replace variable references:
   ```markdown
   # Before
   Session: ${SESSION_NAME}
   Progress file: ${PROGRESS_FILE}

   # After
   Read context from: ${CTX}
   Progress file: ${PROGRESS}
   Write status to: ${STATUS}
   ```

2. Add status output section (see Phase 2.4)

#### 6.3 Remove Deprecated Code

**Files to modify:**
- `scripts/lib/resolve.sh` - Remove old variable resolution
- `scripts/lib/parse.sh` - Deprecate (keep for reference, mark as unused)
- `scripts/engine.sh` - Remove `LOOP_OUTPUT_PARSE` handling

#### 6.4 Update Documentation

**Files to update:**
- `CLAUDE.md` - Update architecture section, variable list, schema examples
- `skills/*/SKILL.md` - Update any references to old variables
- `README.md` - If exists, update examples

#### Success Criteria - Phase 6

- [x] All 5 stage definitions use new schema
- [x] All 4 prompts use `${CTX}`, `${PROGRESS}`, `${STATUS}`
- [x] Old variables work but log deprecation warning
- [x] CLAUDE.md reflects v3 architecture
- [x] Full test: Run each stage type successfully with new format

---

## Risk Analysis

### High Risk

| Risk | Mitigation |
|------|------------|
| Breaking existing sessions | Keep deprecated variables working during migration |
| Agents not writing status.json | Engine creates error status if missing |
| Prompt changes break agent behavior | Test each prompt update individually |

### Medium Risk

| Risk | Mitigation |
|------|------------|
| Pipeline input resolution bugs | Comprehensive tests for `select: all` vs `select: latest` |
| Context.json schema changes | Version the schema, validate on read |
| Crash recovery regression | Test resume after each phase |

### Low Risk

| Risk | Mitigation |
|------|------------|
| Documentation drift | Update docs as part of each phase |
| Old code paths remaining | Lint for deprecated patterns |

---

## Testing Strategy

### Test-Driven Development Workflow

For each phase, follow this pattern:

```
1. WRITE FAILING TESTS
   â””â”€â”€ Add validation rules for new schema
   â””â”€â”€ Create fixtures for expected behavior
   â””â”€â”€ Run: ./scripts/run.sh lint --v3 (should fail)

2. IMPLEMENT CHANGES
   â””â”€â”€ Make code changes
   â””â”€â”€ Update affected files

3. VERIFY TESTS PASS
   â””â”€â”€ Run: ./scripts/run.sh lint (all rules pass)
   â””â”€â”€ Run: ./scripts/run.sh test loop <name> --iterations 2
   â””â”€â”€ Run: ./scripts/run.sh dry-run loop <name> test

4. REGRESSION CHECK
   â””â”€â”€ Run: ./scripts/run.sh lint (all loops/pipelines pass)
   â””â”€â”€ Test: existing sessions still work
```

### Unit Tests (per phase)

| Phase | Test Command | What It Validates |
|-------|--------------|-------------------|
| 0 | `./scripts/run.sh test loop work --mock` | Mock execution works |
| 1 | `./scripts/run.sh dry-run loop work test \| grep context.json` | Context file generated |
| 2 | `cat fixtures/status.json \| jq '.decision'` | Status schema valid |
| 3 | `ls .claude/pipeline-runs/test/iterations/` | Output snapshots exist |
| 4 | `./scripts/run.sh dry-run pipeline test-inputs` | Input resolution works |
| 5 | `./scripts/run.sh status crashed-session` | Failure state readable |

### Integration Tests (Mock Mode)

```bash
# Test single-stage loop with mock Claude
./scripts/run.sh test loop work --iterations 3

# Test plateau detection with mock responses
./scripts/run.sh test loop improve-plan --iterations 5

# Test pipeline with mock execution
./scripts/run.sh test pipeline full-refine.yaml --mock

# Verify context.json generated correctly
./scripts/run.sh test loop work --iterations 1 && \
  cat .claude/pipeline-runs/test/stage-00-work/iterations/001/context.json | jq .
```

### Integration Tests (Live Mode)

```bash
# Test single-stage loop (requires beads)
./scripts/run.sh work test-v3 3

# Test multi-stage pipeline
./scripts/run.sh pipeline full-refine.yaml test-v3

# Test crash recovery
./scripts/run.sh work test-crash 5
# Kill mid-iteration (Ctrl+C or kill)
./scripts/run.sh work test-crash 5 --resume

# Test input selection
./scripts/run.sh pipeline test-inputs.yaml test-inputs
```

### Smoke Test Checklist

After each phase, verify:

- [ ] `./scripts/run.sh lint` passes (0 errors)
- [ ] `./scripts/run.sh dry-run loop work test` shows expected output
- [ ] Existing sessions (`./scripts/run.sh status <name>`) still readable
- [ ] New files created in expected locations

### Regression Test Checklist (End of Phase 6)

- [ ] `work` stage: beads claimed, implemented, closed
- [ ] `improve-plan` stage: plan improved, plateau reached
- [ ] `elegance` stage: exploration runs, consensus reached
- [ ] `idea-wizard` stage: ideas generated, fixed-n terminates
- [ ] Pipeline: multiple stages chain correctly
- [ ] Resume: crashed session continues cleanly
- [ ] Context: `context.json` readable by agent
- [ ] Status: `status.json` written by agent, parsed by engine

---

## Implementation Order

```
Phase 0: Test Harness (TDD Foundation)
  â”œâ”€â”€ 0.1 Add v3 validation rules to scripts/lib/validate.sh
  â”œâ”€â”€ 0.2 Create scripts/lib/mock.sh for mock execution
  â”œâ”€â”€ 0.3 Add test command to scripts/run.sh
  â””â”€â”€ 0.4 Create fixtures/ directories for each loop
  â””â”€â”€ CHECKPOINT: ./scripts/run.sh lint && ./scripts/run.sh test loop work --mock

Phase 1: Context Manifest
  â”œâ”€â”€ 1.1 Create scripts/lib/context.sh
  â”œâ”€â”€ 1.2 Update scripts/lib/resolve.sh
  â””â”€â”€ 1.3 Update scripts/engine.sh (context generation)
  â””â”€â”€ CHECKPOINT: ./scripts/run.sh dry-run loop work test | grep context.json

Phase 2: Universal Status
  â”œâ”€â”€ 2.1 Create scripts/lib/status.sh
  â”œâ”€â”€ 2.2 Update completion strategies
  â”œâ”€â”€ 2.3 Update stage schemas (one loop as pilot)
  â””â”€â”€ 2.4 Update prompt template (pilot loop only)
  â””â”€â”€ CHECKPOINT: ./scripts/run.sh test loop improve-plan --mock --iterations 3

Phase 3: Engine-Side Snapshots
  â”œâ”€â”€ 3.1 Update engine iteration loop
  â””â”€â”€ 3.2 Remove output.mode handling
  â””â”€â”€ CHECKPOINT: ls .claude/pipeline-runs/test/*/iterations/*/output.md

Phase 4: Explicit Inputs
  â”œâ”€â”€ 4.1 Add input resolution to context.sh
  â””â”€â”€ 4.2 Update pipeline schema
  â””â”€â”€ CHECKPOINT: ./scripts/run.sh dry-run pipeline full-refine.yaml test

Phase 5: Fail Fast
  â”œâ”€â”€ 5.1 Remove retry logic
  â”œâ”€â”€ 5.2 Update guardrails schema
  â””â”€â”€ 5.3 Improve failure state
  â””â”€â”€ CHECKPOINT: ./scripts/run.sh status test-crash (after simulated failure)

Phase 6: Cleanup & Migration
  â”œâ”€â”€ 6.1 Update all stage definitions to v3 schema
  â”œâ”€â”€ 6.2 Update all prompts to use ${CTX}, ${STATUS}
  â”œâ”€â”€ 6.3 Remove deprecated code paths
  â””â”€â”€ 6.4 Update CLAUDE.md documentation
  â””â”€â”€ CHECKPOINT: ./scripts/run.sh lint && full regression test
```

### Recommended Order Within Each Phase

1. **Write tests first** (validation rules, fixtures)
2. **Verify tests fail** (expected behavior not yet implemented)
3. **Implement smallest change** that makes test pass
4. **Run checkpoint** to verify phase complete
5. **Run full lint** to catch regressions

---

## File Change Summary

### New Files (Phase 0)
- `scripts/lib/mock.sh` - Mock execution for testing
- `scripts/loops/work/fixtures/default.txt` - Mock response fixture
- `scripts/loops/work/fixtures/status.json` - Expected status schema
- `scripts/loops/improve-plan/fixtures/` - Plateau mock fixtures
- `scripts/loops/elegance/fixtures/` - Elegance mock fixtures
- `scripts/loops/idea-wizard/fixtures/` - Idea wizard mock fixtures

### New Files (Phases 1-5)
- `scripts/lib/context.sh` - Context manifest generator
- `scripts/lib/status.sh` - Status file management

### Major Modifications
- `scripts/engine.sh` - Context generation, status reading, snapshot saving
- `scripts/lib/resolve.sh` - Simplified to 4 variables
- `scripts/lib/validate.sh` - Add v3 validation rules (L014-L017)
- `scripts/lib/completions/plateau.sh` - Read from status.json
- `scripts/run.sh` - Add `test` command

### Minor Modifications
- `scripts/lib/state.sh` - Enhanced failure state
- `scripts/lib/completions/beads-empty.sh` - Status integration
- `scripts/lib/completions/fixed-n.sh` - Status integration
- All `scripts/loops/*/loop.yaml` - New schema
- All `scripts/loops/*/prompt.md` - New variables, status output
- All `scripts/pipelines/*.yaml` - Input selection

### Deprecated (keep for reference)
- `scripts/lib/parse.sh` - No longer needed with status.json

---

## Success Criteria (Overall)

1. **One context interface** - Agents read `context.json` via `${CTX}`
2. **One status format** - Every agent writes identical `status.json`
3. **Automatic history** - Engine saves iteration outputs without config
4. **Preserved learning** - Progress file unchanged
5. **Explicit inputs** - Stages declare `inputs.from` and `inputs.select`
6. **Fail fast** - Failures stop immediately with clear state
7. **Schema = documentation** - No implicit behaviors

When all phases complete: agents can create new pipelines by copying existing stage definitions and modifying only the prompt.
